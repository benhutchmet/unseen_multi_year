{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process UNSEEN ###\n",
    "\n",
    "But in notebook form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 23.11.0\n",
      "    latest version: 24.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /apps/jasmin/jaspy/miniforge_envs/jaspy3.11/mf3-23.11.0-0\n",
      "\n",
      "  added / updated specs:\n",
      "    - iris\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cartopy-0.24.0             |  py310h5eaa309_0         1.3 MB  conda-forge\n",
      "    cf-units-3.2.0             |  py310hf462985_6         293 KB  conda-forge\n",
      "    cftime-1.6.4               |  py310hf462985_1         243 KB  conda-forge\n",
      "    cloudpickle-3.1.0          |     pyhd8ed1ab_1          25 KB  conda-forge\n",
      "    contourpy-1.3.0            |  py310h3788b33_2         255 KB  conda-forge\n",
      "    dask-core-2024.10.0        |     pyhd8ed1ab_0         879 KB  conda-forge\n",
      "    fonttools-4.54.1           |  py310h89163eb_1         2.2 MB  conda-forge\n",
      "    fsspec-2024.10.0           |     pyhff2d567_0         132 KB  conda-forge\n",
      "    geos-3.13.0                |       h5888daf_0         1.8 MB  conda-forge\n",
      "    hdf5-1.14.3                |nompi_h4f84152_100         3.7 MB  conda-forge\n",
      "    importlib_metadata-8.5.0   |       hd8ed1ab_0           9 KB  conda-forge\n",
      "    kiwisolver-1.4.7           |  py310h3788b33_0          70 KB  conda-forge\n",
      "    libblas-3.9.0              |25_linux64_openblas          15 KB  conda-forge\n",
      "    libcblas-3.9.0             |25_linux64_openblas          15 KB  conda-forge\n",
      "    libexpat-2.6.3             |       h5888daf_0          72 KB  conda-forge\n",
      "    libgfortran-14.2.0         |       h69a702a_1          53 KB  conda-forge\n",
      "    libgfortran-ng-14.2.0      |       h69a702a_1          53 KB  conda-forge\n",
      "    libgfortran5-14.2.0        |       hd5240d6_1         1.4 MB  conda-forge\n",
      "    liblapack-3.9.0            |25_linux64_openblas          15 KB  conda-forge\n",
      "    libopenblas-0.3.28         |pthreads_h94d23a6_0         5.3 MB  conda-forge\n",
      "    markupsafe-3.0.2           |  py310h89163eb_0          22 KB  conda-forge\n",
      "    matplotlib-base-3.9.2      |  py310h68603db_1         6.7 MB  conda-forge\n",
      "    netcdf4-1.6.5              |nompi_py310h3aa39b3_102         537 KB  conda-forge\n",
      "    numpy-1.26.4               |  py310hb13e2d6_0         6.7 MB  conda-forge\n",
      "    pillow-10.3.0              |  py310hf73ecf8_0        39.8 MB  conda-forge\n",
      "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
      "    pyparsing-3.2.0            |     pyhd8ed1ab_1          90 KB  conda-forge\n",
      "    pyproj-3.6.1               |  py310hd5c30f3_5         513 KB  conda-forge\n",
      "    python-xxhash-3.5.0        |  py310ha75aee5_1          22 KB  conda-forge\n",
      "    pyyaml-6.0.2               |  py310ha75aee5_1         178 KB  conda-forge\n",
      "    qhull-2020.2               |       h434a139_5         540 KB  conda-forge\n",
      "    scipy-1.14.1               |  py310hfcf56fc_1        16.1 MB  conda-forge\n",
      "    shapely-2.0.6              |  py310had3dfd6_2         476 KB  conda-forge\n",
      "    snappy-1.2.1               |       ha2e4443_0          41 KB  conda-forge\n",
      "    sqlite-3.44.2              |       h2c6b66d_0         817 KB  conda-forge\n",
      "    toolz-1.0.0                |     pyhd8ed1ab_0          51 KB  conda-forge\n",
      "    unicodedata2-15.1.0        |  py310ha75aee5_1         359 KB  conda-forge\n",
      "    xorg-libxau-1.0.11         |       hb9d3cd8_1          14 KB  conda-forge\n",
      "    xorg-libxdmcp-1.1.5        |       hb9d3cd8_0          19 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        90.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  antlr-python-runt~ conda-forge/noarch::antlr-python-runtime-4.11.1-pyhd8ed1ab_0 \n",
      "  blosc              conda-forge/linux-64::blosc-1.21.5-hc2324a3_1 \n",
      "  brotli             conda-forge/linux-64::brotli-1.1.0-hd590300_1 \n",
      "  brotli-bin         conda-forge/linux-64::brotli-bin-1.1.0-hd590300_1 \n",
      "  cartopy            conda-forge/linux-64::cartopy-0.24.0-py310h5eaa309_0 \n",
      "  cf-units           conda-forge/linux-64::cf-units-3.2.0-py310hf462985_6 \n",
      "  cftime             conda-forge/linux-64::cftime-1.6.4-py310hf462985_1 \n",
      "  click              conda-forge/noarch::click-8.1.7-unix_pyh707e725_0 \n",
      "  cloudpickle        conda-forge/noarch::cloudpickle-3.1.0-pyhd8ed1ab_1 \n",
      "  contourpy          conda-forge/linux-64::contourpy-1.3.0-py310h3788b33_2 \n",
      "  cycler             conda-forge/noarch::cycler-0.12.1-pyhd8ed1ab_0 \n",
      "  dask-core          conda-forge/noarch::dask-core-2024.10.0-pyhd8ed1ab_0 \n",
      "  fonttools          conda-forge/linux-64::fonttools-4.54.1-py310h89163eb_1 \n",
      "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
      "  fsspec             conda-forge/noarch::fsspec-2024.10.0-pyhff2d567_0 \n",
      "  geos               conda-forge/linux-64::geos-3.13.0-h5888daf_0 \n",
      "  hdf4               conda-forge/linux-64::hdf4-4.2.15-h2a13503_7 \n",
      "  hdf5               conda-forge/linux-64::hdf5-1.14.3-nompi_h4f84152_100 \n",
      "  importlib-metadata conda-forge/noarch::importlib-metadata-8.5.0-pyha770c72_0 \n",
      "  importlib_metadata conda-forge/noarch::importlib_metadata-8.5.0-hd8ed1ab_0 \n",
      "  iris               conda-forge/noarch::iris-3.10.0-pyha770c72_1 \n",
      "  jinja2             conda-forge/noarch::jinja2-3.1.4-pyhd8ed1ab_0 \n",
      "  kiwisolver         conda-forge/linux-64::kiwisolver-1.4.7-py310h3788b33_0 \n",
      "  lcms2              conda-forge/linux-64::lcms2-2.16-hb7c19ff_0 \n",
      "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n",
      "  libaec             conda-forge/linux-64::libaec-1.1.3-h59595ed_0 \n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-25_linux64_openblas \n",
      "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hd590300_1 \n",
      "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hd590300_1 \n",
      "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hd590300_1 \n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-25_linux64_openblas \n",
      "  libdeflate         conda-forge/linux-64::libdeflate-1.20-hd590300_0 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.6.3-h5888daf_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
      "  libgfortran        conda-forge/linux-64::libgfortran-14.2.0-h69a702a_1 \n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-14.2.0-h69a702a_1 \n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-14.2.0-hd5240d6_1 \n",
      "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.0.0-hd590300_1 \n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-25_linux64_openblas \n",
      "  libnetcdf          conda-forge/linux-64::libnetcdf-4.9.2-nompi_h9612171_113 \n",
      "  libopenblas        conda-forge/linux-64::libopenblas-0.3.28-pthreads_h94d23a6_0 \n",
      "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
      "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
      "  libtiff            conda-forge/linux-64::libtiff-4.6.0-h1dd3fc0_3 \n",
      "  libudunits2        conda-forge/linux-64::libudunits2-2.2.28-h40f5838_3 \n",
      "  libwebp-base       conda-forge/linux-64::libwebp-base-1.4.0-hd590300_0 \n",
      "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
      "  libzip             conda-forge/linux-64::libzip-1.10.1-h2629f0a_3 \n",
      "  locket             conda-forge/noarch::locket-1.0.0-pyhd8ed1ab_0 \n",
      "  markupsafe         conda-forge/linux-64::markupsafe-3.0.2-py310h89163eb_0 \n",
      "  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.9.2-py310h68603db_1 \n",
      "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
      "  netcdf4            conda-forge/linux-64::netcdf4-1.6.5-nompi_py310h3aa39b3_102 \n",
      "  numpy              conda-forge/linux-64::numpy-1.26.4-py310hb13e2d6_0 \n",
      "  openjpeg           conda-forge/linux-64::openjpeg-2.5.2-h488ebb8_0 \n",
      "  partd              conda-forge/noarch::partd-1.4.2-pyhd8ed1ab_0 \n",
      "  pillow             conda-forge/linux-64::pillow-10.3.0-py310hf73ecf8_0 \n",
      "  proj               conda-forge/linux-64::proj-9.3.1-h1d62c97_0 \n",
      "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
      "  pyparsing          conda-forge/noarch::pyparsing-3.2.0-pyhd8ed1ab_1 \n",
      "  pyproj             conda-forge/linux-64::pyproj-3.6.1-py310hd5c30f3_5 \n",
      "  pyshp              conda-forge/noarch::pyshp-2.3.1-pyhd8ed1ab_0 \n",
      "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0-pyhd8ed1ab_0 \n",
      "  python-xxhash      conda-forge/linux-64::python-xxhash-3.5.0-py310ha75aee5_1 \n",
      "  pyyaml             conda-forge/linux-64::pyyaml-6.0.2-py310ha75aee5_1 \n",
      "  qhull              conda-forge/linux-64::qhull-2020.2-h434a139_5 \n",
      "  scipy              conda-forge/linux-64::scipy-1.14.1-py310hfcf56fc_1 \n",
      "  shapely            conda-forge/linux-64::shapely-2.0.6-py310had3dfd6_2 \n",
      "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 \n",
      "  snappy             conda-forge/linux-64::snappy-1.2.1-ha2e4443_0 \n",
      "  sqlite             conda-forge/linux-64::sqlite-3.44.2-h2c6b66d_0 \n",
      "  toolz              conda-forge/noarch::toolz-1.0.0-pyhd8ed1ab_0 \n",
      "  udunits2           conda-forge/linux-64::udunits2-2.2.28-h40f5838_3 \n",
      "  unicodedata2       conda-forge/linux-64::unicodedata2-15.1.0-py310ha75aee5_1 \n",
      "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hb9d3cd8_1 \n",
      "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb9d3cd8_0 \n",
      "  xxhash             conda-forge/linux-64::xxhash-0.8.2-hd590300_0 \n",
      "  yaml               conda-forge/linux-64::yaml-0.2.5-h7f98852_2 \n",
      "  zipp               conda-forge/noarch::zipp-3.20.2-pyhd8ed1ab_0 \n",
      "  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.8.30-hbcca054_0 \n",
      "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.8.30-pyhd8ed1ab_0 \n",
      "  libgcc-ng                               13.2.0-h807b86a_3 --> 14.2.0-h69a702a_1 \n",
      "  libgomp                                 13.2.0-h807b86a_3 --> 14.2.0-h77fa898_1 \n",
      "  openssl                                  3.2.0-hd590300_1 --> 3.3.2-hb9d3cd8_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'iris'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshapereader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mshpreader\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01miris\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Specific imports\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'iris'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Local imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shapely.geometry\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import iris\n",
    "\n",
    "# Specific imports\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load my specific functions\n",
    "sys.path.append(\"/home/users/benhutch/unseen_functions\")\n",
    "import functions as funcs\n",
    "import bias_adjust as ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Hardcoded variables\n",
    "model = \"HadGEM3-GC31-MM\"\n",
    "experiment = \"dcppA-hindcast\"\n",
    "freq = \"Amon\" # go back to using monthly data\n",
    "\n",
    "# Set up the arguments\n",
    "variable = \"tas\"\n",
    "country = \"United_Kingdom\"\n",
    "season = \"ONDJFM\"\n",
    "first_year = 1960\n",
    "last_year = 2018\n",
    "model_fcst_year = 1\n",
    "lead_year = \"1-10\"\n",
    "detrend = True # True for temperature, false for wind speeds\n",
    "bias_correct = \"None\" # No bias correction for tas months\n",
    "percentile = 10\n",
    "\n",
    "# Save directory\n",
    "save_dir = \"/gws/nopw/j04/canari/users/benhutch/plots/unseen\"\n",
    "\n",
    "# list of valid bias corrections\n",
    "valid_bias_corrections = [\n",
    "    \"None\",\n",
    "    \"linear_scaling\",\n",
    "    \"variance_scaling\",\n",
    "    \"quantile_mapping\",\n",
    "    \"quantile_delta_mapping\",\n",
    "    \"scaled_distribution_mapping\",\n",
    "]\n",
    "\n",
    "# Set up the output directory for the dfs\n",
    "output_dir_dfs = \"/gws/nopw/j04/canari/users/benhutch/unseen/saved_dfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# if the bias correction is not in the valid bias corrections\n",
    "if bias_correct not in valid_bias_corrections:\n",
    "    raise ValueError(f\"Bias correction {bias_correct} not recognised\")\n",
    "\n",
    "# set up the obs variable depending on the variable\n",
    "if variable == \"tas\":\n",
    "    obs_var = \"t2m\"\n",
    "elif variable == \"sfcWind\":\n",
    "    obs_var = \"si10\"\n",
    "else:\n",
    "    raise ValueError(\"Variable not recognised\")\n",
    "\n",
    "# Set up the months depending on the season\n",
    "if season == \"DJF\":\n",
    "    months = [12, 1, 2]\n",
    "elif season == \"NDJ\":\n",
    "    months = [11, 12, 1]\n",
    "elif season == \"OND\":\n",
    "    months = [10, 11, 12]\n",
    "elif season == \"JFM\":\n",
    "    months = [1, 2, 3]\n",
    "elif season == \"MAM\":\n",
    "    months = [3, 4, 5]\n",
    "elif season == \"JJA\":\n",
    "    months = [6, 7, 8]\n",
    "elif season == \"SON\":\n",
    "    months = [9, 10, 11]\n",
    "elif season == \"ONDJFM\":\n",
    "    months = [10, 11, 12, 1, 2, 3]\n",
    "elif season == \"NDJFM\":\n",
    "    months = [11, 12, 1, 2, 3]\n",
    "else:\n",
    "    raise ValueError(\"Season not recognised\")\n",
    "\n",
    "# Depending on the model forecast year\n",
    "# set the leads to extract from the model\n",
    "if model_fcst_year == 0 and season == \"NDJFM\":\n",
    "    lead_months = [1, 2, 3, 4, 5]\n",
    "elif model_fcst_year == 1 and season == \"ONDJFM\":\n",
    "    lead_months = [12, 13, 14, 15, 16, 17]\n",
    "elif model_fcst_year == 1 and season in [\"OND\", \"NDJ\", \"DJF\", \"JFM\"]:\n",
    "    lead_months = [12, 13, 14, 15, 16, 17] # include all then subset later\n",
    "else:\n",
    "    raise ValueError(\"Model forecast year and season not recognised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up the name for the obs df\n",
    "obs_df_name = f\"ERA5_obs_{variable}_{country}_{season}_{first_year}_{last_year}.csv\"\n",
    "\n",
    "# Set up the name for the model df\n",
    "model_df_name = f\"{model}_{variable}_{country}_{season}_{first_year}_{last_year}_{experiment}_{freq}.csv\"\n",
    "\n",
    "# form the full paths for the dfs\n",
    "obs_df_path = os.path.join(output_dir_dfs, obs_df_name)\n",
    "model_df_path = os.path.join(output_dir_dfs, model_df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# if the obs df exists and the model df exists\n",
    "if os.path.exists(obs_df_path) and os.path.exists(model_df_path):\n",
    "    print(\"Loading the observed and model dfs\")\n",
    "\n",
    "    # load the dfs\n",
    "    obs_df = pd.read_csv(obs_df_path)\n",
    "    model_df = pd.read_csv(model_df_path)\n",
    "\n",
    "    # print(\"Loaded the dfs\")\n",
    "    # print(\"----------------\")\n",
    "    # print(\"Script complete\")\n",
    "else:\n",
    "    print(\"Creating the observed and model dfs\")\n",
    "    # Set up the path to the ERA5 data\n",
    "    # if the variable is tas\n",
    "    if variable == \"tas\":\n",
    "        # already regridded!\n",
    "        obs_path = (\n",
    "            \"/gws/nopw/j04/canari/users/benhutch/ERA5/t2m_ERA5_regrid_HadGEM.nc\"\n",
    "        )\n",
    "    # if the variable is sfcWind\n",
    "    elif variable == \"sfcWind\":\n",
    "        # needs regridding\n",
    "        obs_path = \"/gws/nopw/j04/canari/users/benhutch/ERA5/surface_wind_ERA5.nc\"\n",
    "    else:\n",
    "        raise ValueError(\"Variable not recognised\")\n",
    "\n",
    "    # Load the model ensemble\n",
    "    model_ds = funcs.load_model_data_xarray(\n",
    "        model_variable=variable,\n",
    "        model=model,\n",
    "        experiment=experiment,\n",
    "        start_year=first_year,\n",
    "        end_year=last_year,\n",
    "        first_fcst_year=int(first_year) + 1,\n",
    "        last_fcst_year=int(first_year) + 2,\n",
    "        months=months,\n",
    "        frequency=freq,\n",
    "        parallel=False,\n",
    "    )\n",
    "\n",
    "    # print that we have loaded the model data\n",
    "    print(\"Loaded the model data\")\n",
    "\n",
    "    # # Get the size of the model data in bytes\n",
    "    # size_in_bytes = model_ds[variable].size * model_ds[variable].dtype.itemsize\n",
    "\n",
    "    # # Convert bytes to gigabytes\n",
    "    # size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "    # # Print the size\n",
    "    # print(f\"Model data size: {size_in_gb} GB\")\n",
    "\n",
    "    # Modify member coordiante before conbersion to iris\n",
    "    model_ds[\"member\"] = model_ds[\"member\"].str[1:-6].astype(int)\n",
    "\n",
    "    # convert to an iris cube\n",
    "    model_cube = model_ds[variable].squeeze().to_iris()\n",
    "\n",
    "    # Load the observed data\n",
    "    obs_ds = xr.open_mfdataset(\n",
    "        obs_path,\n",
    "        combine=\"by_coords\",\n",
    "        parallel=False,\n",
    "        engine=\"netcdf4\",\n",
    "    )\n",
    "\n",
    "    # Restrict the time to the region we are interested in\n",
    "    obs_ds = obs_ds.sel(\n",
    "        time=slice(\n",
    "            f\"{int(first_year)}-{months[0]}-01\",\n",
    "            f\"{int(last_year) + 1}-{months[-1]}-31\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # If expver is present in the observations\n",
    "    if \"expver\" in obs_ds.coords:\n",
    "        # Combine the first two expver variables\n",
    "        obs_ds = obs_ds.sel(expver=1).combine_first(obs_ds.sel(expver=5))\n",
    "\n",
    "    # # Get the size of the observed data in bytes\n",
    "    # size_in_bytes = obs_ds[obs_var].size * obs_ds[obs_var].dtype.itemsize\n",
    "\n",
    "    # # Convert bytes to gigabytes\n",
    "    # size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "    # # Print the size\n",
    "    # print(f\"Observed data size: {size_in_gb} GB\")\n",
    "\n",
    "    # convert to an iris cube\n",
    "    obs_cube = obs_ds[obs_var].squeeze().to_iris()\n",
    "\n",
    "    # if the lats and lons are not the same\n",
    "    if (\n",
    "        not model_cube.coord(\"latitude\").shape == obs_cube.coord(\"latitude\").shape\n",
    "        or not model_cube.coord(\"longitude\").shape\n",
    "        == obs_cube.coord(\"longitude\").shape\n",
    "    ):\n",
    "        print(\"Regridding model data\")\n",
    "        # regrid the obs cube to the model cube\n",
    "        obs_cube = obs_cube.regrid(model_cube, iris.analysis.Linear())\n",
    "\n",
    "    # make sure the cubes are correct in -180 to 180 lons\n",
    "    obs_cube = obs_cube.intersection(longitude=(-180, 180))\n",
    "    model_cube = model_cube.intersection(longitude=(-180, 180))\n",
    "\n",
    "    # create the mask\n",
    "    MASK_MATRIX = funcs.create_masked_matrix(\n",
    "        country=country,\n",
    "        cube=model_cube,\n",
    "    )\n",
    "\n",
    "    # print the shape of the mask matrix\n",
    "    print(f\"Mask matrix shape: {MASK_MATRIX.shape}\")\n",
    "\n",
    "    # print the sum of the mask matrix\n",
    "    print(f\"Mask matrix sum: {np.sum(MASK_MATRIX)}\")\n",
    "\n",
    "    # Apply the mask to the observed data\n",
    "    obs_values = obs_cube.data * MASK_MATRIX\n",
    "    model_values = model_cube.data * MASK_MATRIX\n",
    "\n",
    "    # Where there are zeros in the mask we want to set these to Nans\n",
    "    obs_values_masked = np.where(MASK_MATRIX == 0, np.nan, obs_values)\n",
    "    model_values_masked = np.where(MASK_MATRIX == 0, np.nan, model_values)\n",
    "\n",
    "    # Take the Nanmean of the data\n",
    "    obs_values = np.nanmean(obs_values_masked, axis=(1, 2))\n",
    "    model_values = np.nanmean(model_values_masked, axis=(3, 4))\n",
    "\n",
    "    # Set up the ref time for the observations\n",
    "    ref_time_obs = datetime(1900, 1, 1)\n",
    "\n",
    "    # Extract the obs time points\n",
    "    obs_time_points = obs_cube.coord(\"time\").points\n",
    "\n",
    "    # convert to obs datetimes\n",
    "    obs_datetimes = [\n",
    "        ref_time_obs + timedelta(hours=int(tp)) for tp in obs_time_points\n",
    "    ]\n",
    "\n",
    "    # Set up a dataframe for the observations\n",
    "    obs_df = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": obs_datetimes,\n",
    "            \"obs\": obs_values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # set up an empty df for the model data\n",
    "    model_df = pd.DataFrame()\n",
    "\n",
    "    # extract the init, member and lead time points\n",
    "    init_years = model_cube.coord(\"init\").points\n",
    "    members = model_cube.coord(\"member\").points\n",
    "    lead_times = model_cube.coord(\"lead\").points\n",
    "\n",
    "    # loop through the inits, members and leadtimes\n",
    "    for i, init_year in enumerate(init_years):\n",
    "        for m, member in enumerate(members):\n",
    "            for l, lead_time in enumerate(lead_times):\n",
    "                # get the model data\n",
    "                model_data = model_values[i, m, l]\n",
    "\n",
    "                # set up the model df this\n",
    "                model_df_this = pd.DataFrame(\n",
    "                    {\n",
    "                        \"init_year\": [init_year],\n",
    "                        \"member\": [member],\n",
    "                        \"lead\": [lead_time],\n",
    "                        \"data\": [model_data],\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # concat to the model df\n",
    "                model_df = pd.concat([model_df, model_df_this])\n",
    "\n",
    "    # print the head of the obs df\n",
    "    print(obs_df.head())\n",
    "\n",
    "    # print the head of the model df\n",
    "    print(model_df.head())\n",
    "\n",
    "    # save the dfs\n",
    "    if not os.path.exists(output_dir_dfs):\n",
    "        os.makedirs(output_dir_dfs)\n",
    "\n",
    "    # save the obs df\n",
    "    if not os.path.exists(obs_df_path):\n",
    "        print(\"Saving the observed df\")\n",
    "        obs_df.to_csv(obs_df_path, index=False)\n",
    "\n",
    "    # save the model df\n",
    "    if not os.path.exists(model_df_path):\n",
    "        print(\"Saving the model df\")\n",
    "        model_df.to_csv(model_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# constrain the obs df to only months 10, 11, 12, 1, 2, 3\n",
    "# esnure that the time is a datetime\n",
    "obs_df[\"time\"] = pd.to_datetime(obs_df[\"time\"])\n",
    "\n",
    "# set the time as the index for the obs df\n",
    "obs_df.set_index(\"time\", inplace=True)\n",
    "\n",
    "# # remove the name of the index\n",
    "# obs_df.index.name = None\n",
    "\n",
    "# print the head of the obs df\n",
    "print(obs_df.head())\n",
    "\n",
    "# constrain to the months\n",
    "obs_df = obs_df[obs_df.index.month.isin(months)]\n",
    "\n",
    "# NOTE: Not taking ONDJFM averages\n",
    "# if months contains 12, 1 in sequence\n",
    "# if 12 in months and 1 in months:\n",
    "#     # shift back by months and take the annual mean\n",
    "#     obs_df = obs_df.shift(-int(months[-1])).resample(\"A\").mean()\n",
    "\n",
    "# if there are any Nans in the obs df, drop them\n",
    "obs_df.dropna(inplace=True)\n",
    "\n",
    "# set up time as a column\n",
    "obs_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# create a new model df for subsetting to first ONDJFM\n",
    "model_df_ondjfm = pd.DataFrame()\n",
    "\n",
    "# turn leads into a list of ints\n",
    "if lead_year != \"9999\":\n",
    "    if \"-\" in lead_year:\n",
    "        leads = list(\n",
    "            range(\n",
    "                int(lead_year.split(\"-\")[0]),\n",
    "                int(lead_year.split(\"-\")[1]) + 1,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        leads = [int(lead_year)]\n",
    "\n",
    "    # print the leads to extract\n",
    "    print(f\"Leads to extract: {leads}\")\n",
    "elif lead_year == \"9999\":\n",
    "    # Set up the leads to extract list range 1-10\n",
    "    leads = list(range(1, 11))\n",
    "else:\n",
    "    raise ValueError(\"Lead year not recognised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# loop over the unique init years and members in model_df\n",
    "for init_year in model_df[\"init_year\"].unique():\n",
    "    for member in model_df[\"member\"].unique():\n",
    "        for l in leads:\n",
    "            # extract the model data\n",
    "            model_data = model_df[\n",
    "                (model_df[\"init_year\"] == init_year)\n",
    "                & (model_df[\"member\"] == member)\n",
    "            ]\n",
    "\n",
    "            # create the list of lead months to extract\n",
    "            lead_months_year_base = [l * lead_months[0] for lm in lead_months]\n",
    "\n",
    "            # # print the lead months year base\n",
    "            # print(\"lead months year base:\", lead_months_year_base)\n",
    "\n",
    "            # create the list of lead months to extract\n",
    "            for i in range(len(lead_months_year_base)):\n",
    "                lead_months_year_base[i] = lead_months_year_base[i] + i\n",
    "\n",
    "            # # print the lead months year base\n",
    "            # print(\"lead months year base:\", lead_months_year_base)\n",
    "\n",
    "            # # subset to lead values [12, 13, 14, 15, 16, 17] and take the mean\n",
    "            # # first complete ONDJFM season\n",
    "            # # FIXME: Hardcoded for now\n",
    "            # model_data = model_data[model_data[\"lead\"].isin(lead_months_year_base)]\n",
    "\n",
    "            # mean_data = model_data[\"data\"].mean()\n",
    "                \n",
    "            # # print lead months year base\n",
    "            # print(\"lead months year base:\", lead_months_year_base)\n",
    "\n",
    "            # loop over the lead months\n",
    "            for lm in lead_months_year_base:\n",
    "                # subset to the lead month\n",
    "                mean_data = model_data[model_data[\"lead\"] == lm].mean()[\"data\"]\n",
    "\n",
    "                # create a dataframe this\n",
    "                model_data_this = pd.DataFrame(\n",
    "                    {\n",
    "                        \"init_year\": [init_year],\n",
    "                        \"member\": [member],\n",
    "                        \"lead\": [lm],\n",
    "                        \"data\": [mean_data],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                model_df_ondjfm = pd.concat([model_df_ondjfm, model_data_this])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# if the detrend is True\n",
    "if detrend and bias_correct == \"None\":\n",
    "    print(\"Detrending the data, no bias correction\")\n",
    "\n",
    "    # apply the function to detrend the data\n",
    "    obs_df, model_df_ondjfm = funcs.apply_detrend(\n",
    "        obs_df=obs_df,\n",
    "        model_df=model_df_ondjfm,\n",
    "        obs_val_name=\"obs\",\n",
    "        model_val_name=\"data\",\n",
    "        obs_time_name=\"time\",\n",
    "        model_time_name=\"init_year\",\n",
    "        model_member_name=\"member\",\n",
    "        model_lead_name=\"lead\",\n",
    "    )\n",
    "\n",
    "    # Set up the name for the obs val name\n",
    "    obs_val_name = \"obs_dt\"\n",
    "    model_val_name = \"data_dt\"\n",
    "elif bias_correct != \"None\" and not detrend:\n",
    "    print(\"Bias correcting the data, no detrending\")\n",
    "\n",
    "    # if the bias correction is linear_scaling\n",
    "    if bias_correct == \"linear_scaling\":\n",
    "        # apply the function to bias correct the data\n",
    "        model_df_ondjfm = funcs.bc_linear_scaling(\n",
    "            obs_df=obs_df,\n",
    "            model_df=model_df_ondjfm,\n",
    "            obs_val_name=\"obs\",\n",
    "            model_val_name=\"data\",\n",
    "        )\n",
    "    elif bias_correct == \"variance_scaling\":\n",
    "        # apply the function to bias correct the data\n",
    "        model_df_ondjfm = funcs.bc_variance_scaling(\n",
    "            obs_df=obs_df,\n",
    "            model_df=model_df_ondjfm,\n",
    "            obs_val_name=\"obs\",\n",
    "            model_val_name=\"data\",\n",
    "        )\n",
    "    elif bias_correct == \"quantile_mapping\":\n",
    "        # Use James functions to correct the model data\n",
    "        qm_adjustment = ba.QMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_bc\"] = qm_adjustment.correct()\n",
    "    elif bias_correct == \"quantile_delta_mapping\":\n",
    "        # Use James functions to correct the model data\n",
    "        qdm_adjustment = ba.QDMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_bc_qdm\"] = qdm_adjustment.correct()\n",
    "\n",
    "        # compare to the quantile mapping adjustment\n",
    "        qm_adjustment = ba.QMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_bc_qm\"] = qm_adjustment.correct()\n",
    "\n",
    "        # take the difference between the two columns\n",
    "        model_df_ondjfm[\"data_bc_diff\"] = model_df_ondjfm[\"data_bc_qm\"] - model_df_ondjfm[\"data_bc_qdm\"]\n",
    "\n",
    "        # print the head of the model df\n",
    "        print(model_df_ondjfm.head())\n",
    "\n",
    "        # print the tail of the model df\n",
    "        print(model_df_ondjfm.tail())\n",
    "    elif bias_correct == \"scaled_distribution_mapping\":\n",
    "        print(\"Applying scaled distribution mapping\")\n",
    "\n",
    "        sdm_adjustment = ba.SDMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_bc\"] = sdm_adjustment.correct()\n",
    "    else:\n",
    "        print(f\"Bias correction method {bias_correct} not recognised\")\n",
    "\n",
    "    # Set up the name for the obs val name\n",
    "    obs_val_name = \"obs\"\n",
    "    model_val_name = \"data_bc\"\n",
    "\n",
    "    # print the mean bias\n",
    "    print(\n",
    "        \"Mean bias:\",\n",
    "        np.mean(model_df_ondjfm[model_val_name]) - np.mean(obs_df[obs_val_name]),\n",
    "    )\n",
    "\n",
    "    # print the spread bias\n",
    "    print(\n",
    "        \"Spread bias:\",\n",
    "        np.std(model_df_ondjfm[model_val_name]) - np.std(obs_df[obs_val_name]),\n",
    "    )\n",
    "\n",
    "elif bias_correct != \"None\" and detrend:\n",
    "    print(\"Bias correcting the data and detrending\")\n",
    "\n",
    "    # apply the function to detrend the data\n",
    "    obs_df, model_df_ondjfm = funcs.apply_detrend(\n",
    "        obs_df=obs_df,\n",
    "        model_df=model_df_ondjfm,\n",
    "        obs_val_name=\"obs\",\n",
    "        model_val_name=\"data\",\n",
    "        obs_time_name=\"time\",\n",
    "        model_time_name=\"init_year\",\n",
    "        model_member_name=\"member\",\n",
    "        model_lead_name=\"lead\",\n",
    "    )\n",
    "\n",
    "    # # print the mean of the model data\n",
    "    # print(\"Model data mean before bias correction:\", np.mean(model_df_ondjfm[\"data_dt\"]))\n",
    "\n",
    "    # # print the spread of the model data\n",
    "    # print(\"Model data spread before bias correction:\", np.std(model_df_ondjfm[\"data_dt\"]))\n",
    "\n",
    "    if bias_correct == \"linear_scaling\":\n",
    "        # apply the function to bias correct the data\n",
    "        model_df_ondjfm = funcs.bc_linear_scaling(\n",
    "            obs_df=obs_df,\n",
    "            model_df=model_df_ondjfm,\n",
    "            obs_val_name=\"obs_dt\",\n",
    "            model_val_name=\"data_dt\",\n",
    "        )\n",
    "    elif bias_correct == \"variance_scaling\":\n",
    "        # apply the function to bias correct the data\n",
    "        model_df_ondjfm = funcs.bc_variance_scaling(\n",
    "            obs_df=obs_df,\n",
    "            model_df=model_df_ondjfm,\n",
    "            obs_val_name=\"obs_dt\",\n",
    "            model_val_name=\"data_dt\",\n",
    "        )\n",
    "    elif bias_correct == \"quantile_mapping\":\n",
    "        # use James' functions to correct the model data\n",
    "        qm_adjustment = ba.QMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data_dt\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_dt_bc\"] = qm_adjustment.correct()\n",
    "    elif bias_correct == \"quantile_delta_mapping\":\n",
    "        # Use James functions to correct the model data\n",
    "        qdm_adjustment = ba.QDMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data_dt\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_dt_bc\"] = qdm_adjustment.correct()\n",
    "    elif bias_correct == \"scaled_distribution_mapping\":\n",
    "        print(\"Applying scaled distribution mapping\")\n",
    "\n",
    "        sdm_adjustment = ba.SDMBiasAdjust(\n",
    "            obs_data = obs_df[\"obs\"],\n",
    "            mod_data = model_df_ondjfm[\"data_dt\"],\n",
    "        )\n",
    "\n",
    "        # assign the corrected data to the model df\n",
    "        model_df_ondjfm[\"data_dt_bc\"] = sdm_adjustment.correct()\n",
    "    else:\n",
    "        print(f\"Bias correction method {bias_correct} not recognised\")\n",
    "        sys.exit()\n",
    "\n",
    "    # # print the mean of the model data\n",
    "    # print(\"Model data mean after bias correction:\", np.mean(model_df_ondjfm[\"data_dt_bc\"]))\n",
    "\n",
    "    # # print the spread of the model data\n",
    "    # print(\"Model data spread after bias correction:\", np.std(model_df_ondjfm[\"data_dt_bc\"]))\n",
    "\n",
    "    # # print the observed mean\n",
    "    # print(\"Observed data mean before bias correction:\", np.mean(obs_df[\"obs_dt\"]))\n",
    "\n",
    "    # # print the spread of the observed data\n",
    "    # print(\"Observed data spread before bias correction:\", np.std(obs_df[\"obs_dt\"]))\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # Set up the name for the obs val name\n",
    "    obs_val_name = \"obs_dt\"\n",
    "    model_val_name = \"data_dt_bc\"\n",
    "\n",
    "else:\n",
    "    obs_val_name = \"obs\"\n",
    "    model_val_name = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# print the obs val name being used\n",
    "print(\"----------------\")\n",
    "print(f\"Obs val name: {obs_val_name}\")\n",
    "print(f\"Model val name: {model_val_name}\")\n",
    "print(\"----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
